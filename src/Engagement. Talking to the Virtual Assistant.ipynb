{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engagement\n",
    "\n",
    "### Analyzing data from 2018/2019\n",
    "\n",
    "* Measuring whether students talked to the chatbot or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = { 'family': 'DejaVu Sans', 'weight': 'bold', 'size': 22 }\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grades = pd.read_json(os.path.join(data_path, 'grades.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3217"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of students that talked to the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from students import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_index(e1, e2):\n",
    "    return float(e2 - e1) / e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_gain_index(e1, e2):\n",
    "    if e1 == 0 and e2 == 0:\n",
    "        return 0\n",
    "    elif e1 == 0:\n",
    "        return 1\n",
    "    gi = gain_index(e1, e2)\n",
    "    if gi > 1:\n",
    "        return 1\n",
    "    return gi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: CA116, Academic year: 2018/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_course = 'ca116'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_academic_year = (2018, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grades(course, academic_year):\n",
    "    return df_grades[(df_grades['module'] == course) & \n",
    "                     (df_grades['academic_year_0'] == academic_year[0]) &\n",
    "                     (df_grades['academic_year_1'] == academic_year[1])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_ca116 = df_grades.iloc[get_grades(_course, _academic_year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grades_ca116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_names_ca116 = grades_ca116.user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(student_names_ca116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_weeks_ca116 = sorted(grades_ca116.exam_week.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 12]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam_weeks_ca116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "talked_to_bot_ca116 = data.CA116_2018_2019_STUDENTS_VIRTUAL_ASSISTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(talked_to_bot_ca116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_talked_to_bot_ca116 = [student for student in student_names_ca116 if student not in talked_to_bot_ca116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_talked_to_bot_ca116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(student_dict, normgi, diff, marks, exam_weeks):\n",
    "    \n",
    "    # Store improvements\n",
    "    d = {}\n",
    "    \n",
    "    for name, students in student_dict.items():\n",
    "        \n",
    "        num_students = len(students)\n",
    "        \n",
    "        print('Group: {} (# {})'.format(name, num_students))\n",
    "        \n",
    "        for exam_week in exam_weeks:\n",
    "            # Marks\n",
    "            mrks = [ value for student_name, value in marks[exam_week].items() if student_name in students ]    \n",
    "            # Avg\n",
    "            mrks_avg = np.mean(mrks)\n",
    "            # Std. deviation\n",
    "            mrks_std = np.std(mrks)\n",
    "            print('Marks Exam Week {}, Avg: {:.2f} ({:.2f})'.format(exam_week, mrks_avg, mrks_std))\n",
    "        \n",
    "        # Array of improvements\n",
    "        improvements = [ value for student_name, value in normgi.items() if student_name in students ]\n",
    "        d[name] = improvements\n",
    "        \n",
    "        # Avg\n",
    "        improv_avg = np.mean(improvements)\n",
    "        # Std. deviation\n",
    "        improv_std = np.std(improvements)\n",
    "        \n",
    "        # Differences\n",
    "        differences = [ value for student_name, value in diff.items() if student_name in students ]\n",
    "        \n",
    "        # Avg\n",
    "        diff_avg = np.mean(differences)\n",
    "        # Std. deviation\n",
    "        diff_std = np.std(differences)\n",
    "              \n",
    "        print('* Improvement Avg: {:.2f} ({:.2f}), Diff Avg: {:.2f} ({:.2f})'.format(\n",
    "            mrks_avg, mrks_std, improv_avg, improv_std, diff_avg, diff_std))\n",
    "        \n",
    "    # T-test\n",
    "    keys = list(student_dict.keys())\n",
    "    group_one = d[keys[0]]\n",
    "    group_two = d[keys[1]]\n",
    "    ttest = ttest_ind(group_one, group_two)\n",
    "    print(ttest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_dict_ca116 = {\n",
    "    'Talked': talked_to_bot_ca116,\n",
    "    'Not-talked': not_talked_to_bot_ca116,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_improvement(student_names, exam_weeks, grades, debug=False):\n",
    "    \n",
    "    marks = {}\n",
    "    normgi = {}\n",
    "    diff = {}\n",
    "    \n",
    "    for student_name in student_names:\n",
    "        \n",
    "        prev = None\n",
    "        improvement = None\n",
    "        \n",
    "        for exam_week in exam_weeks:\n",
    "            \n",
    "            grade = 0\n",
    "            grade_index = grades[(grades['user'] == student_name) &\n",
    "                                 (grades['exam_week'] == exam_week)].index\n",
    "            if len(grade_index) > 0:\n",
    "                grade = df_grades.iloc[grade_index[0]]['grade']\n",
    "                \n",
    "            marks.setdefault(exam_week, {})\n",
    "            marks[exam_week][student_name] = grade\n",
    "            \n",
    "            if debug: print('Student: {}, Exam week: {}, Grade: {}'.format(student_name, exam_week, grade))\n",
    "            \n",
    "            if prev is not None:\n",
    "                improvement = normalized_gain_index(prev, grade)\n",
    "                if debug: print('Student: {}, Improvement: {:.2f}'.format(student_name, improvement))\n",
    "                normgi[student_name] = improvement\n",
    "                diff[student_name] = grade - prev\n",
    "                \n",
    "            prev = grade\n",
    "            \n",
    "    return normgi, diff, marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "normgi_ca116, diff_ca116, marks_ca116 = get_improvement(student_names_ca116, exam_weeks_ca116, grades_ca116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(student_dict, normgi, diff, marks, exam_weeks):\n",
    "    \n",
    "    # Store improvements\n",
    "    d = {}\n",
    "    \n",
    "    for name, students in student_dict.items():\n",
    "        \n",
    "        num_students = len(students)\n",
    "        \n",
    "        print('Group: {} (# {})'.format(name, num_students))\n",
    "        \n",
    "        for exam_week in exam_weeks:\n",
    "            # Marks\n",
    "            mrks = [ value for student_name, value in marks[exam_week].items() if student_name in students ]    \n",
    "            # Avg\n",
    "            mrks_avg = np.mean(mrks)\n",
    "            # Std. deviation\n",
    "            mrks_std = np.std(mrks)\n",
    "            print('Marks Exam Week {}, Avg: {:.2f} ({:.2f})'.format(exam_week, mrks_avg, mrks_std))\n",
    "        \n",
    "        # Array of improvements\n",
    "        improvements = [ value for student_name, value in normgi.items() if student_name in students ]\n",
    "        d[name] = improvements\n",
    "        \n",
    "        # Avg\n",
    "        improv_avg = np.mean(improvements)\n",
    "        # Std. deviation\n",
    "        improv_std = np.std(improvements)\n",
    "        \n",
    "        # Differences\n",
    "        differences = [ value for student_name, value in diff.items() if student_name in students ]\n",
    "        \n",
    "        # Avg\n",
    "        diff_avg = np.mean(differences)\n",
    "        # Std. deviation\n",
    "        diff_std = np.std(differences)\n",
    "              \n",
    "        print('* Improvement Avg: {:.2f} ({:.2f}), Diff Avg: {:.2f} ({:.2f})'.format(\n",
    "            mrks_avg, mrks_std, improv_avg, improv_std, diff_avg, diff_std))\n",
    "        \n",
    "    # T-test\n",
    "    keys = list(student_dict.keys())\n",
    "    group_one = d[keys[0]]\n",
    "    group_two = d[keys[1]]\n",
    "    ttest = ttest_ind(group_one, group_two)\n",
    "    print(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: Talked (# 52)\n",
      "Marks Exam Week 4, Avg: 70.19 (31.79)\n",
      "Marks Exam Week 8, Avg: 47.12 (35.58)\n",
      "Marks Exam Week 12, Avg: 33.65 (34.63)\n",
      "* Improvement Avg: 33.65 (34.63), Diff Avg: -0.18 (0.50)\n",
      "Group: Not-talked (# 80)\n",
      "Marks Exam Week 4, Avg: 72.50 (33.45)\n",
      "Marks Exam Week 8, Avg: 45.62 (37.24)\n",
      "Marks Exam Week 12, Avg: 32.50 (35.66)\n",
      "* Improvement Avg: 32.50 (35.66), Diff Avg: -0.21 (0.60)\n",
      "Ttest_indResult(statistic=0.27004486080036505, pvalue=0.7875539555896455)\n"
     ]
    }
   ],
   "source": [
    "evaluate(bot_dict_ca116, normgi_ca116, diff_ca116, marks_ca116, exam_weeks_ca116)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
